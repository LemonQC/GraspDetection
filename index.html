<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/GraspDetection/img/favicon-32x32.png">
    <title>GraspDetection</title>
    
<link rel="stylesheet" href="/GraspDetection/css/reset.css">

    
<link rel="stylesheet" href="/GraspDetection/css/style.css">

    
<link rel="stylesheet" href="/GraspDetection/css/markdown.css">

    
<link rel="stylesheet" href="/GraspDetection/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/GraspDetection/2023/10/12/paper/">Hierarchical Multi-modal Fusion for Language-driven Robotic Grasping Detection in Clutter</a> -->
        <div class="post-title" >Hierarchical Multi-modal Fusion for Language-driven Robotic Grasping Detection in Clutter</div>
        <div class="post-affiliations" >Shandong University</div>
        <div class="head-icon"> 

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://github.com/LemonQC/Seq2Seq2GraspAndLocalization">
                    <i class="fa fa-github-square"></i>
                </a>
            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/P1DN4fkxVic">
                    <i class="fa fa-youtube-square"></i>
                </a>
            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Language-driven grasping detection in clutter represents a pivotal and formidable milestone in realizing human-machine collaboration. Existing methods typically rely on well-trained object detectors to localize single object, after which grasping postures are generated based on the identified object. However, this sequential approach can lead to error accumulation. Moreover, it lacks the capability to comprehend sentence logic and spatial relationships of objects, resulting in inconsistency results between object grasping detection and object localization. Consequently, identifying and grasping the queried object for human interaction become challenging. In this paper, we propose an end-to-end object localization and grasping detection network. Specifically, we first extract salient objects and spatial relationships from sentence logic and employ CLIP to acquire both visual and textual features. Subsequently, we introduce different vision-language fusion modules to conduct multi-modal fusions at the object-level, spatial-level, and global-level aspects, respectively. Conditioned on the multi-modal features, we further design a hierarchical feature modeling mechanism that integrates the fused features from the above levels to achieve simultaneous object localization and accurate grasping detection. Extensive experiments on the real-world datasets and robotic applications demonstrate the effectiveness and accuracy of our proposed framework.</div>
        <div class="post-video-title">
            <i class="fa fa-hand-o-right"></i><a  class= 'post-video-title' target="_blank" rel="noopener" href="https://youtu.be/P1DN4fkxVic">Video on YouTube</a><i class="fa fa-hand-o-left"></i>
            
        </div>
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/P1DN4fkxVic"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h2 id="Language-driven-Grasping-Detection-Pipeline-Comparison"><a href="#Language-driven-Grasping-Detection-Pipeline-Comparison" class="headerlink" title="Language-driven Grasping Detection Pipeline Comparison"></a>Language-driven Grasping Detection Pipeline Comparison</h2><p>Existing works typically rely on well-trained object detectors to localize single object, after which grasp postures are generated based on the identified object. However, this sequential approach can lead to error accumulation. Our proposed pipeline can accurately locate the queried object and predict the grasp posture, ensuring that the network truly comprehend the queries and implement the grasping operations.<br><img src="/GraspDetection/./images/pipelines.jpg" alt="Pipeline comparison"></p>
<h2 id="Our-Detailed-Language-driven-Grasping-Detection-Pipeline"><a href="#Our-Detailed-Language-driven-Grasping-Detection-Pipeline" class="headerlink" title="Our Detailed Language-driven Grasping Detection Pipeline"></a>Our Detailed Language-driven Grasping Detection Pipeline</h2><p>An overview of the proposed end-to-end robotic grasping detection and position localization system. The system is conditioned on human instructions to identify and grasp a particular object within a cluttered environment.<br><img src="/GraspDetection/./images/ourpipeline.jpg" alt="OurPipeline"></p>
<h2 id="Overview-of-ELGNet"><a href="#Overview-of-ELGNet" class="headerlink" title="Overview of ELGNet"></a>Overview of ELGNet</h2><p>An overview of ELGNet architecture.  Conditioned on the extracted three-aspect visual features, we propose different fusion strategies to perceive the visual context according to the query within clutter environment.  Then, we develop a hierarchical fusion modeling and joint optimization strategy that simultaneously determines object location and grasping posture.<br><img src="/GraspDetection/./images/model.jpg" alt="listen-perceive-grasp paradigm"></p>
<h3 id="Multi-aspect-Fusion-Modules"><a href="#Multi-aspect-Fusion-Modules" class="headerlink" title="Multi-aspect Fusion Modules"></a>Multi-aspect Fusion Modules</h3><p>Three different fusion strategies.The first strategy is global-level feature fusion (GFF), which aims to understand the complex scenario context with high compressed semantic visual information. The second strategy is object-level feature fusion (OFF), which can ensure mining of object information effectively. The last strategy focus on spatial feature fusion (SFF) between the low-level spatial visual features and spatial-level linguistic information to obtain spatial context required for final object location.<br><img src="/GraspDetection/./images/fusionmodules.jpg" alt="TAMMI"></p>
<h2 id="Dataset-examples"><a href="#Dataset-examples" class="headerlink" title="Dataset examples"></a>Dataset examples</h2><p><font color=black size=4> Several grasping examples in three clutter</font></p>
<p><img src="/GraspDetection/./images/examples.jpg" alt="Dataset Examples"></p>
<h2 id="Qualitative-results"><a href="#Qualitative-results" class="headerlink" title="Qualitative results"></a>Qualitative results</h2><p><font color=black size=4> 1. Comparison results in different clutter scenes</font></p>
<p><img src="/GraspDetection/./images/compare.jpg" alt="Visualization"></p>
<p><font color=black size=4>2. Qualitative results of our proposed method in different clutter sceness</font></p>
<p><img src="/GraspDetection/./images/selfcompare.jpg" alt="Visualization2"></p>

            <!-- <a class="read-more" href="/GraspDetection/2023/10/12/paper/"> ... </a> -->
        </div>
        <div class="post-date">2023.10.12</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 GraspDetection</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/GraspDetection/css/a11y-dark.min.css">


<script src="/GraspDetection/js/highlight.min.js"></script>


<script src="/GraspDetection/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>
