<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/GraspDetection/img/favicon-32x32.png">
    <title>GraspDetection</title>
    
<link rel="stylesheet" href="/GraspDetection/css/reset.css">

    
<link rel="stylesheet" href="/GraspDetection/css/style.css">

    
<link rel="stylesheet" href="/GraspDetection/css/markdown.css">

    
<link rel="stylesheet" href="/GraspDetection/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-main">

    
    
        <div class="post-main-title">
            Listen, Perceive, Grasp : CLIP-driven attribute-aware network for visual-language segmentation and grasping detection
        </div>
        <div class="post-meta">
            2023-10-12
        </div>
        <div class="post-md">
            <h2 id="Language-driven-Grasping-Detection-Pipeline-Comparison"><a href="#Language-driven-Grasping-Detection-Pipeline-Comparison" class="headerlink" title="Language-driven Grasping Detection Pipeline Comparison"></a>Language-driven Grasping Detection Pipeline Comparison</h2><p>Existing works typically rely on well-trained object detectors to localize single object, after which grasp postures are generated based on the identified object. However, this sequential approach can lead to error accumulation. Our proposed pipeline can accurately locate the queried object and predict the grasp posture<br><img src="/GraspDetection/./images/pipelines.jpg" alt="Pipeline comparison"></p>
<h2 id="Our-Detailed-Language-driven-Grasping-Detection-Pipeline"><a href="#Our-Detailed-Language-driven-Grasping-Detection-Pipeline" class="headerlink" title="Our Detailed Language-driven Grasping Detection Pipeline"></a>Our Detailed Language-driven Grasping Detection Pipeline</h2><p>An overview of the proposed end-to-end robotic grasping detection and position localization system. The system is conditioned on human instructions to identify and grasp a particular object within a cluttered environment.<br><img src="/GraspDetection/./images/ourpipeline.jpg" alt="OurPipeline"></p>
<h2 id="Overview-of-ELGNet"><a href="#Overview-of-ELGNet" class="headerlink" title="Overview of ELGNet"></a>Overview of ELGNet</h2><p>An overview of CTNet architecture. Given an image-query pair, we first employ listen module to understand and align visual and linguistic concepts. Then, the perceive module recursively extracts object-orientied features with visual attributes and generates the target mask corresponding to the description. Last,  The grasp module aggregates the attribute-based and semantic-rich features to refine the grasp pose of the desired object.<br><img src="/GraspDetection/./images/model.jpg" alt="listen-perceive-grasp paradigm"></p>
<h3 id="Multi-aspect-Fusion-Modules"><a href="#Multi-aspect-Fusion-Modules" class="headerlink" title="Multi-aspect Fusion Modules"></a>Multi-aspect Fusion Modules</h3><p><img src="/GraspDetection/./images/fusionmodules.jpg" alt="TAMMI"></p>
<h2 id="Dataset-examples"><a href="#Dataset-examples" class="headerlink" title="Dataset examples"></a>Dataset examples</h2><p>**Several grasping examples in three clutter **<br><img src="/GraspDetection/./images/examples.jpg" alt="Dataset Examples"></p>
<h2 id="Qualitative-results"><a href="#Qualitative-results" class="headerlink" title="Qualitative results"></a>Qualitative results</h2><p><strong>Comparison results in different clutter scenes.</strong></p>
<p><img src="/GraspDetection/./images/compare.jpg" alt="Visualization"></p>
<p><strong>Qualitative results of our proposed method in different clutter scenes.</strong></p>
<p><img src="/GraspDetection/./images/selfcompare.jpg" alt="Visualization2"></p>

        </div>

    

</div>
                <div class="footer">
    <span>Copyright Â© 2023 GraspDetection</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/GraspDetection/css/a11y-dark.min.css">


<script src="/GraspDetection/js/highlight.min.js"></script>


<script src="/GraspDetection/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>